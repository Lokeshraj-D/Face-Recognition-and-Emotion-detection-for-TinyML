{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e18d83f-0070-42b9-8194-dc626ec2a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4912a9f-87ca-48ec-ab6c-58ae66e3a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c829883-525f-42a8-8e9c-e78c8939eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_json_file = \"model.json\"\n",
    "model_weights_file = \"emotion_detect.h5\"\n",
    "with open(model_json_file, \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    classifier = model_from_json(loaded_model_json)\n",
    "    classifier.load_weights(model_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adfaec2-4638-4efa-a97b-06a3634bd632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('FR_prune_LRF.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1c208a-4535-4d00-a201-c4c59d2fe9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in c:\\users\\dlr22\\anaconda3\\envs\\py310\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in c:\\users\\dlr22\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in c:\\users\\dlr22\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow-model-optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in c:\\users\\dlr22\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow-model-optimization) (1.24.3)\n",
      "Requirement already satisfied: six~=1.14 in c:\\users\\dlr22\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow-model-optimization) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a904644c-1ab9-4f79-b301-56d9a2ce79ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "tfmot.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b6497-bf6c-4584-ab8a-b60c0bc0a726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61cc741-0e65-4353-9ef5-d7cb4ddcee4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please initialize `Prune` with a supported layer. Layers should either be supported by the PruneRegistry (built-in keras layers) or should be a `PrunableLayer` instance, or should has a customer defined `get_prunable_weights` method. You passed: <class 'keras.layers.core.tf_op_layer.TFOpLambda'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_for_pruning \u001b[38;5;241m=\u001b[39m \u001b[43mtfmot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparsity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_low_magnitude\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     73\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_FAILURE_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     68\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     results \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_SUCCESS_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\sparsity\\keras\\prune.py:211\u001b[0m, in \u001b[0;36mprune_low_magnitude\u001b[1;34m(to_prune, pruning_schedule, block_size, block_pooling_type, pruning_policy, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m pruning_policy:\n\u001b[0;32m    210\u001b[0m     pruning_policy\u001b[38;5;241m.\u001b[39mensure_model_supports_pruning(to_prune)\n\u001b[1;32m--> 211\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_add_pruning_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_prune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_keras_layer:\n\u001b[0;32m    213\u001b[0m   params\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\sparsity\\keras\\prune.py:182\u001b[0m, in \u001b[0;36mprune_low_magnitude.<locals>._add_pruning_wrapper\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    178\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m layer\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    179\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential)):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubclassed models are not supported currently.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 182\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_add_pruning_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, pruning_wrapper\u001b[38;5;241m.\u001b[39mPruneLowMagnitude):\n\u001b[0;32m    185\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m layer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\models\\cloning.py:529\u001b[0m, in \u001b[0;36mclone_model\u001b[1;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, functional\u001b[38;5;241m.\u001b[39mFunctional):\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;66;03m# If the get_config() method is the same as a regular Functional\u001b[39;00m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# model, we're safe to use _clone_functional_model (which relies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;66;03m# or input_tensors are passed, we attempt it anyway\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;66;03m# in order to preserve backwards compatibility.\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mis_default(model\u001b[38;5;241m.\u001b[39mget_config) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    527\u001b[0m         clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors\n\u001b[0;32m    528\u001b[0m     ):\n\u001b[1;32m--> 529\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_functional_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_function\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Case of a custom model class\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\models\\cloning.py:212\u001b[0m, in \u001b[0;36m_clone_functional_model\u001b[1;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(layer_fn):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `layer_fn` argument to be a callable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: layer_fn=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m model_configs, created_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_layers_and_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_input_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Reconstruct model from the config, using the cloned layers.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m (\n\u001b[0;32m    217\u001b[0m     input_tensors,\n\u001b[0;32m    218\u001b[0m     output_tensors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m     model_configs, created_layers\u001b[38;5;241m=\u001b[39mcreated_layers\n\u001b[0;32m    222\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\models\\cloning.py:288\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config\u001b[1;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[0;32m    285\u001b[0m         created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m layer_fn(layer)\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m--> 288\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize_layer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_copy_layer\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config, created_layers\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\functional.py:1577\u001b[0m, in \u001b[0;36mget_network_config\u001b[1;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[0;32m   1572\u001b[0m         node_data \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mserialize(\n\u001b[0;32m   1573\u001b[0m             _make_node_key, node_conversion_map\n\u001b[0;32m   1574\u001b[0m         )\n\u001b[0;32m   1575\u001b[0m         filtered_inbound_nodes\u001b[38;5;241m.\u001b[39mappend(node_data)\n\u001b[1;32m-> 1577\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1578\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1579\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minbound_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_inbound_nodes\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\models\\cloning.py:285\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config.<locals>._copy_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    283\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m InputLayer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mget_config())\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\sparsity\\keras\\prune.py:189\u001b[0m, in \u001b[0;36mprune_low_magnitude.<locals>._add_pruning_wrapper\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    187\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pruning_wrapper\u001b[38;5;241m.\u001b[39mPruneLowMagnitude(layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\sparsity\\keras\\pruning_wrapper.py:191\u001b[0m, in \u001b[0;36mPruneLowMagnitude.__init__\u001b[1;34m(self, layer, pruning_schedule, block_size, block_pooling_type, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m   \u001b[38;5;28msuper\u001b[39m(PruneLowMagnitude, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    189\u001b[0m       prune_registry\u001b[38;5;241m.\u001b[39mPruneRegistry\u001b[38;5;241m.\u001b[39mmake_prunable(layer), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease initialize `Prune` with a supported layer. Layers should \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    193\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meither be supported by the PruneRegistry (built-in keras layers) or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    194\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould be a `PrunableLayer` instance, or should has a customer \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    195\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefined `get_prunable_weights` method. You passed: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    196\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m))\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_track_trackable(layer, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# TODO(yunluli): Work-around to handle the first layer of Sequential model\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# properly. Can remove this when it is implemented in the Wrapper base\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# class.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# built. Being not built is confusing since the end-user has passed an\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# input shape.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Please initialize `Prune` with a supported layer. Layers should either be supported by the PruneRegistry (built-in keras layers) or should be a `PrunableLayer` instance, or should has a customer defined `get_prunable_weights` method. You passed: <class 'keras.layers.core.tf_op_layer.TFOpLambda'>"
     ]
    }
   ],
   "source": [
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d778d5b-9652-4cc6-a98f-ee8a98cfd4cd",
   "metadata": {},
   "source": [
    "## Laod dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2172da1c-e948-4df8-996f-3491cdeed9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5e2864-9e48-4c95-9b98-eb087230358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def produce_dataset(num_classes):\n",
    "    images_set = []\n",
    "    labels = []\n",
    "    for i in range(num_classes):\n",
    "        data_path = os.path.join(\"data\", f\"{str(i)}\")\n",
    "        for images in os.listdir(data_path):\n",
    "            img = cv2.imread(os.path.join(data_path, images))\n",
    "            faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                roi_color = img[y:y+h, x:x+w]\n",
    "                resized_array = cv2.resize(roi_color, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "                images_set.append(resized_array)\n",
    "                labels.append(i+1)\n",
    "\n",
    "    return images_set, labels\n",
    "\n",
    "num_of_classes = 4\n",
    "X, y = produce_dataset(num_of_classes) \n",
    "label_bin = LabelBinarizer()\n",
    "y = label_bin.fit_transform(y)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "test_size = 0.10\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcdeeb9-1dbd-44e3-b666-c57be0cc073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# building data generator \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 128\n",
    "base_path = \"images/images/\"\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255.0,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1,\n",
    "                                   rotation_range = 20,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(os.path.join(base_path, \"train\"),\n",
    "                                                    target_size=(56,56),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(os.path.join(base_path, \"validation\"),\n",
    "                                                    target_size=(56,56),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278a8b4-9ecb-4de6-89e5-d69bee025cd4",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e8490b0-aad5-463a-bca2-1d34c3c6c8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_modelfer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_modelfer/assets\n"
     ]
    }
   ],
   "source": [
    "classifier.save(\"./saved_modelfer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab9e855-9d5b-4d31-8e77-e0bb30079045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_prunes_LRF/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_prunes_LRF/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./saved_prunes_LRF/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d66cdf1-967a-4e4c-beb2-cb3a8bd7d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for data in tf.data.Dataset.from_tensor_slices(X_test).batch(1).take(100):\n",
    "        yield [tf.cast(data, tf.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8a40eb-7cb5-4d83-89d4-0af05f148732",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_prunes_LRF\")\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0305b821-df45-478a-ba97-87122f61cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_prunes_LRF\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5c4955-a278-4705-80ff-d128f9c50359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4036248"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10c2021-92e3-4fd3-9760-ebb3c021b5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294336"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d17b330c-cc23-4ea0-9db6-da38d4272be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_model_fer.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5af81df1-b686-4b3b-a559-450ddbc7a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_quant_model_fer.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5decbb20-f23c-4d97-a719-3f0a67c83064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization_to_dense(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Dense):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae707099-830c-4dbd-a22e-199bb212703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization_to_dense,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad661cb-18bb-4e4e-81fe-0c8b6ef0ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed242a-21c7-408d-9b63-0374aee6bfee",
   "metadata": {},
   "source": [
    "## Training Quantization aware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba13c8b-47c1-46f7-a4a5-d727c9f33377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images generator 34\n",
      "Validation images generator: 8\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True, \n",
    "            rotation_range=15,\n",
    "            fill_mode = 'nearest')\n",
    "\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = datagen.flow(X_train, \n",
    "                               y_train, \n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True\n",
    "                               )\n",
    "\n",
    "validation_generator = datagen_val.flow(X_val,\n",
    "                                        y_val,\n",
    "                                       batch_size = batch_size,\n",
    "                                       shuffle = True)\n",
    "\n",
    "print (\"train images generator\",len(train_generator))\n",
    "print(\"Validation images generator:\", len(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceb6ad44-4b99-4f8d-87e0-1417bc598978",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=True,\n",
    "    name='Adam',\n",
    ")\n",
    "\n",
    "quant_aware_model.compile(optimizer=optimizer1,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0070211f-b76f-42a5-88df-1b1a4ee80517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 15s 243ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "total time 15.470043897628784 second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "\n",
    "quant_aware_model.fit(train_generator,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=1,\n",
    "                    shuffle = True,\n",
    "                    validation_data= validation_generator)\n",
    "\n",
    "print(\"total time\", time.time()-start,\"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "918884cc-eb52-4bf1-b134-741286d48cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 208ms/step - loss: 0.0029 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0028632930479943752, 1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "442788a2-71cc-4325-94a1-2a5bc9ed917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.\n",
    "def representative_data_gen():\n",
    "    for input_data in tf.data.Dataset.from_tensor_slices(X_train.astype(\"float32\")).batch(1).take(100):\n",
    "        yield [input_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d584fda7-c1a0-4d86-bcbc-28a37f2e754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 61). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlr22\\AppData\\Local\\Temp\\tmpbo4lagul\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlr22\\AppData\\Local\\Temp\\tmpbo4lagul\\assets\n",
      "C:\\Users\\dlr22\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8  # or tf.float16\n",
    "converter.inference_output_type = tf.uint8  # or tf.float16\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_qaware_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d434a2-f800-4ea4-bfeb-e5174ee9cebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294584"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_qaware_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bdb7234-1ac0-4c57-8fb7-b1d251a1c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_qaware_model.tflite\", 'wb') as f:\n",
    "    f.write(tflite_qaware_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c196cc78-573c-414f-8e85-596c2b983fec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tensorflow/lite/kernels/kernel_util.cc:320 scale_diff / output_scale <= 0.02 was not true.Node number 110 (FULLY_CONNECTED) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtflite_qaware_model.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mInterpreter(model_path\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m----> 5\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocate_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get tensor details\u001b[39;00m\n\u001b[0;32m      8\u001b[0m input_details \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_input_details()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:507\u001b[0m, in \u001b[0;36mInterpreter.allocate_tensors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocate_tensors\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    506\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 507\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAllocateTensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tensorflow/lite/kernels/kernel_util.cc:320 scale_diff / output_scale <= 0.02 was not true.Node number 110 (FULLY_CONNECTED) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_path = \"tflite_qaware_model.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "# Calculate memory usage\n",
    "total_memory = 0\n",
    "for tensor in tensor_details:\n",
    "    tensor_shape = tensor['shape']\n",
    "    tensor_dtype = tensor['dtype']\n",
    "    # Calculate size in bytes\n",
    "    tensor_size = tf.dtypes.as_dtype(tensor_dtype).size * tf.math.reduce_prod(tensor_shape).numpy()\n",
    "    total_memory += tensor_size\n",
    "\n",
    "print(f\"Total memory required for inference: {total_memory / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee1654-1602-4453-9142-f3112462b263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
